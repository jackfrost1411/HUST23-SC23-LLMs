{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6675ed4e-2b7b-4f44-b122-c3223676356b",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944f0369-efc1-40c4-a4a9-40ca31753db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dshah47/.conda/envs/genai23.08_gradio/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "import base64 \n",
    "import requests \n",
    "requests.adapters.DEFAULT_TIMEOUT = 60\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005fb117-4468-4713-839c-3ad7f8c3f8cc",
   "metadata": {},
   "source": [
    "## Setting the path to institute's centralized database containing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ce8b3d-58c2-48e9-b8e1-1fa1741690fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the cache directory - loading the downloaded models\n",
    "# This directory can have all the Large Language Models. The size of the directory could get into terabytes\n",
    "# Currently, we are at 2.7 TiB \n",
    "# Setting this variable, sets the huggingface hub path - reads and writes defaults to this path\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = os.environ['LLM_CACHE_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb2de3-ecc1-4e86-89fc-02cdacd80382",
   "metadata": {},
   "source": [
    "## Loading the LLM from the path specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660c81d-b821-4d14-9a32-4ef4011320e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"tiiuae/falcon-40b-instruct\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             trust_remote_code=True,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             load_in_8bit=True,\n",
    "                                             device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6983c47b-98d6-4b66-bcce-138162bb5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "class CustomStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, stops = []):\n",
    "        self.stops = stops\n",
    "        self.ENCOUNTERS = 2\n",
    "    \n",
    "    def __call__(self, input_ids: torch.LongTensor, score: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop in self.stops:\n",
    "            if stop == tokenizer.decode(input_ids[0][-1]).strip():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([CustomStoppingCriteria([\"\\nUser:\", \"User:\", \"Falcon:\", \"User\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635639a9-9d1c-4d81-a113-85643f435c84",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Integrating memory for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d185b3-722a-4b26-a61f-98cfd03f9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\", max_new_tokens=512, \n",
    "    pad_token_id=tokenizer.eos_token_id, stopping_criteria=stopping_criteria\n",
    ")\n",
    "summary_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 110 (prompt of memory buffer) + 512 (summary words) + 1046 (max length of new generations) < 2046\n",
    "memory = ConversationSummaryBufferMemory(llm=summary_llm, max_token_limit=200)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "\n",
    "conversation_prompt_template='''Below is an instruction that describes a task, paired with current conversation to provide history of conversation and \\\n",
    "an input that provides further context. \\\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are an AI named Falcon. Answer the questions asked to you in a talkative manner.\n",
    "\n",
    "### Current conversation:\n",
    "{history}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d16021a-a9a8-456b-ba6b-93d9004a67ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "print(memory.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9aba5f-7675-4f02-b413-22a0bfcf9107",
   "metadata": {},
   "source": [
    "## Setting up the Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db698fab-f822-465f-baa2-11b71478ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True) #timeout=10.,    \n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\", max_new_tokens=800, \n",
    "                pad_token_id=tokenizer.eos_token_id, stopping_criteria=stopping_criteria, streamer=streamer)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 729 (prompt of memory buffer) + 512 (summary words) + 800 (max length of new generations)\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
    "conversation.prompt.template = conversation_prompt_template\n",
    "\n",
    "def respond(message, chat_history, instruction, temperature=0.7, skip_words=[\"<|endoftext|>\", \"\\nUser:\", \"User:\", \"Falcon:\"]):\n",
    "    chat_history = chat_history + [[message, \"\"]]\n",
    "    \n",
    "    # Start generation on a separate thread, so that we don't block the UI. The text is pulled from the streamer\n",
    "    # in the main thread. Adds timeout to the streamer to handle exceptions in the generation thread.\n",
    "    \n",
    "    generate_kwargs = dict(\n",
    "        input= message,\n",
    "    )\n",
    "    t = Thread(target=conversation.predict, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "    \n",
    "    acc_text = \"\"\n",
    "    #Streaming the tokens\n",
    "    for idx, response in enumerate(streamer):\n",
    "            text_token = response\n",
    "            # if text_token in skip_words:\n",
    "            #     continue\n",
    "\n",
    "            if idx == 0 and text_token.startswith(\" \"):\n",
    "                text_token = text_token[1:]\n",
    "\n",
    "            acc_text += text_token\n",
    "            last_turn = list(chat_history.pop(-1))\n",
    "            last_turn[-1] += acc_text\n",
    "            chat_history = chat_history + [last_turn]\n",
    "            yield \"\", chat_history\n",
    "            acc_text = \"\"\n",
    "    \n",
    "    # Crucial step for summarizing in the memory chain - you have to wait when you go over max allowed tokens for the model to summarize\n",
    "    _ = t.join()\n",
    "\n",
    "gr.close_all()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=600) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    with gr.Accordion(label=\"Advanced options\",open=False):\n",
    "        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n",
    "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n",
    "        max_new_tokens = gr.Slider(label=\"Max New Tokens\", minimum=1, maximum=1000, value=250, step=1, interactive=True)\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "demo.queue().launch(share=True)#, server_port=int(os.environ['PORT4']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai23.08_gradio",
   "language": "python",
   "name": "genai23.08_gradio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
