{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7845a5bd-a65f-46b4-a775-6c2a5b842cc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Cards\n",
    "\n",
    "### This notebook showcases 5 LLMs with varying licenses. To run all models, follow these [GPU Configurations](https://github.com/jackfrost1411/HUST23-SC23-LLMs/tree/master/gpu-config).\n",
    "\n",
    "#### The layout is as follows:\n",
    "\n",
    "* **Initial set up (Important Environment Variable Discussion)**\n",
    "* **Falcon 7b/ 13b/ 40b Instruct (Apache 2.0 License)**\n",
    "* **LLaMa2 7b/ 13b/ 70b (Meta License)**\n",
    "* **StableBeluga 70b (Non Commercial License)**\n",
    "* **MPT 30b (CC-By-SA-3.0)**\n",
    "* **CodeLLaMa 13b/ 34b (Meta License)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b0163-a438-4fbc-a244-914dedecd311",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting the path to institute's centralized database containing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18815d70-04f5-439b-8a54-c281f3f141b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specifying the cache directory - loading the downloaded models\n",
    "# This directory can have all the Large Language Models. The size of the directory could get into terabytes\n",
    "# Currently, we are at 2.7 TiB \n",
    "# Setting this variable, sets the huggingface hub path - reads and writes defaults to this path\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = os.environ['LLM_CACHE_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9357c1b-cf44-4bdc-a13f-6ba0c7906728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the number of GPUs requested\n",
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26610f-e51a-4acd-af64-f7b7d3c902fc",
   "metadata": {
    "id": "WXOZ_Un6e1oo",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##  Falcon 40b Instruct\n",
    "#### It is made available under the Apache 2.0 license.\n",
    "##### For running 7b model change the model_id to \"tiiuae/falcon-7b-instruct\"\n",
    "##### https://huggingface.co/tiiuae/falcon-40b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d458a6dd-8496-42d8-b7fa-347f500e67bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/envs/genai23.08/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████| 9/9 [02:12<00:00, 14.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"tiiuae/falcon-40b-instruct\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             trust_remote_code=True,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             load_in_8bit=True,\n",
    "                                             device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35192b1c-69df-470b-b910-ed6cd7cac28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_length=1048,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65305f5c-6958-4fd7-8648-c279bc95da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory#Summary\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=local_llm, max_token_limit=512)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "conversation = ConversationChain(\n",
    "    llm=local_llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "conversation.prompt.template='''Below is an instruction that describes a task, paired with current conversation to provide history of conversation and \\\n",
    "an input that provides further context. \\\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are an AI named Falcon. Answer the questions asked to you in a talkative manner.\n",
    "\n",
    "### Current conversation:\n",
    "{history}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:'''\n",
    "\n",
    "class ChatBot:\n",
    "    exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
    "\n",
    "    #Method to start the conversation\n",
    "    def start_chat(self):\n",
    "        user_response = input(\"Chat here!\\n\")\n",
    "        while user_response == '':\n",
    "            user_response = input(\"Chat here!\\n\")\n",
    "        self.chat(user_response)\n",
    "\n",
    "    #Method to handle the conversation\n",
    "    def chat(self, reply):\n",
    "        while not self.make_exit(reply):\n",
    "            input_ = reply\n",
    "            reply = input(f\"{conversation.predict(input = input_)}\\n\")\n",
    "\n",
    "    #Method to check for exit commands\n",
    "    def make_exit(self, reply):\n",
    "        for exit_command in self.exit_commands:\n",
    "            if exit_command in reply.lower():\n",
    "                memory.clear()\n",
    "                print(\"Ok, have a great day!\")\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b929428b-5aea-406c-8038-f648eb4cda9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chat here!\n",
      " Hello there\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/envs/genai23.08/lib/python3.11/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/packages/envs/genai23.08/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Hi there! How are you doing? I'm Falcon, an AI language model. What can I help you with?\n",
      " bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok, have a great day!\n"
     ]
    }
   ],
   "source": [
    "chatbot = ChatBot()\n",
    "chatbot.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d163c4-593a-4b02-9418-de76c18dbdd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## LLaMA2 70b Chat\n",
    "#### License A custom commercial license is available at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/\n",
    "##### For running 7b / 13b models change the model_id to \"/scratch/dshah47/.cache/licensed_models/Llama-2-7b-chat-hf/\" / \"/scratch/dshah47/.cache/licensed_models/Llama-2-13b-chat-hf/\"\n",
    "##### https://huggingface.co/meta-llama/Llama-2-70b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c27cb7-f7ca-418c-b81a-791a03acbf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/envs/genai23.08/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████| 15/15 [00:33<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoConfig\n",
    "import torch\n",
    "\n",
    "model_id = f\"{os.environ['HUGGINGFACE_HUB_CACHE']}/licensed_models/Llama-2-70b-chat-hf/\"\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True, use_auth_token=True)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_id,\n",
    "                                         # trust_remote_code=True,\n",
    "                                         torch_dtype=torch.bfloat16,\n",
    "                                         load_in_8bit=True,\n",
    "                                         device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fadcda27-611a-487d-80ac-a5c8b90f532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    pad_token_id=tokenizer.eos_token_id, \n",
    "    max_length=2048,\n",
    "    # temperature=1,\n",
    "    # top_p=0.95,\n",
    "    # repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "713c986a-f8ae-4b66-9a5e-749352d4faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory#Summary\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=local_llm, max_token_limit=512)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "conversation = ConversationChain(\n",
    "    llm=local_llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "conversation.prompt.template='''[INST]<<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
    "<</SYS>>\n",
    "\n",
    "CONTEXT: \n",
    "\n",
    "{history}\n",
    "\n",
    "Question: {input}[/INST]'''\n",
    "\n",
    "class ChatBot:\n",
    "    exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
    "\n",
    "    #Method to start the conversation\n",
    "    def start_chat(self):\n",
    "        user_response = input(\"Chat here!\\n\")\n",
    "        while user_response == '':\n",
    "            user_response = input(\"Chat here!\\n\")\n",
    "        self.chat(user_response)\n",
    "\n",
    "    #Method to handle the conversation\n",
    "    def chat(self, reply):\n",
    "        while not self.make_exit(reply):\n",
    "            input_ = reply\n",
    "            reply = input(f\"{conversation.predict(input = input_)}\\n\")\n",
    "\n",
    "    #Method to check for exit commands\n",
    "    def make_exit(self, reply):\n",
    "        for exit_command in self.exit_commands:\n",
    "            if exit_command in reply.lower():\n",
    "                memory.clear()\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"Ok, have a great day!\")\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6496318-1cfe-4d67-8753-8e80cca7fa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chat here!\n",
      " Hi there my name is Jack Frost\n",
      "  Greetings! It's nice to meet you, Jack Frost. Is there something I can assist you with or would you like to chat for a bit?\n",
      " What do you think my name means?\n",
      "  I'm just an AI, I don't have personal opinions or beliefs, but I can provide some information about the meaning of names. The name \"Jack Frost\" is a popular name that has been associated with various meanings. Some people believe that it refers to the frost that forms on windows during cold weather, while others think it's related to the character Jack Frost from European folklore, who was known for bringing winter weather. However, the actual meaning of the name is uncertain and may vary depending on cultural context and personal interpretation.\n",
      " What's your name then?\n",
      "  My name is Jack Frost.\n",
      " I thought your name was LLaMa\n",
      "  Hello! My apologies for the confusion earlier. My name is indeed LLaMa, and I'm here to help you with any questions or concerns you may have. How can I assist you today?\n",
      " bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok, have a great day!\n"
     ]
    }
   ],
   "source": [
    "chatbot = ChatBot()\n",
    "chatbot.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ca1c6-1f6a-4808-b2a8-166cc7c3aaf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## StableBeluga 70b\n",
    "#### License: Fine-tuned checkpoints (Stable Beluga 2) is licensed under the STABLE BELUGA NON-COMMERCIAL COMMUNITY LICENSE AGREEMENT\n",
    "#### https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt\n",
    "##### Stable Beluga 2 is a Llama2 70B model finetuned on an Orca style Dataset\n",
    "##### https://huggingface.co/stabilityai/StableBeluga2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a7e5e7-f0e6-4a4f-9d3b-557ba3c6737d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/envs/genai23.08/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████| 29/29 [04:51<00:00, 10.05s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"stabilityai/StableBeluga2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"stabilityai/StableBeluga2\", \n",
    "                                             torch_dtype=torch.float16, \n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             load_in_8bit=True,\n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "# system_prompt = \"### System:\\nYou are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal.\\n\\n\"\n",
    "\n",
    "# message = \"Write me a poem please\"\n",
    "# prompt = f\"{system_prompt}### User: {message}\\n\\n### Assistant:\\n\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "# output = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n",
    "\n",
    "# print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b05f53-ca63-4ff7-aea3-8844b8eb9819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_length=1048,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbd4f61a-aa95-41be-a8dd-4994da9f4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory#Summary\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=local_llm, max_token_limit=512)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "conversation = ConversationChain(\n",
    "    llm=local_llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "conversation.prompt.template='''### System:\n",
    "You are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal. Below is the current conversation history and an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Current conversation:\n",
    "{history}\n",
    "\n",
    "### User:\n",
    "{input}\n",
    "\n",
    "### Assistant:'''\n",
    "\n",
    "class ChatBot:\n",
    "    exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
    "\n",
    "    #Method to start the conversation\n",
    "    def start_chat(self):\n",
    "        user_response = input(\"Chat here!\\n\")\n",
    "        while user_response == '':\n",
    "            user_response = input(\"Chat here!\\n\")\n",
    "        self.chat(user_response)\n",
    "\n",
    "    #Method to handle the conversation\n",
    "    def chat(self, reply):\n",
    "        while not self.make_exit(reply):\n",
    "            input_ = reply\n",
    "            reply = input(f\"{conversation.predict(input = input_)}\\n\")\n",
    "\n",
    "    #Method to check for exit commands\n",
    "    def make_exit(self, reply):\n",
    "        for exit_command in self.exit_commands:\n",
    "            if exit_command in reply.lower():\n",
    "                memory.clear()\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"Ok, have a great day!\")\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9f8060-c88d-4e8f-9fb1-f5963f85c73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chat here!\n",
      " Hi there\n",
      "s\n",
      " how are you\n",
      " I am doing well, thank you for asking. How can I assist you today?\n",
      " What is your name\n",
      "\n",
      " My name is Stable Beluga. I am an AI designed to help and assist you with various tasks and answer your questions to the best of my abilities.\n",
      " Yes can you write me a poem on Supercomputers\n",
      "\n",
      " Stable Beluga:\n",
      "\n",
      "Supercomputers, oh what a sight\n",
      "Processing power, beyond our might\n",
      "They crunch numbers, solve complex problems\n",
      "Faster than lightning, no need for thumbs\n",
      "\n",
      "In labs and research centers, they hum and whirr\n",
      "Simulating worlds, both near and far\n",
      "Predicting weather, unlocking secrets\n",
      "Of the universe, no need for regrets\n",
      "\n",
      "Their processors, cores, and memory\n",
      "Work in unison, a symphony\n",
      "Of calculations, beyond our grasp\n",
      "Leaving us humans, in sheer awe, to gasp\n",
      "\n",
      "So let us hail these mighty machines\n",
      "Their power unmatched, like silver screens\n",
      "Supercomputers, our modern age heroes\n",
      "Solving problems, with nary a zero\n",
      "\n",
      "Stable Beluga\n",
      " Thank you\n",
      "\n",
      " You're welcome! If you have any more requests or questions, feel free to ask. I'm here to help.\n",
      " bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok, have a great day!\n"
     ]
    }
   ],
   "source": [
    "chatbot = ChatBot()\n",
    "chatbot.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172fcac3-ad33-4138-95a3-9e85f2b4b52b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MosaicML 30b Instruct\n",
    "#### License: CC-By-SA-3.0 \n",
    "#### This model was trained by MosaicML and follows a modified decoder-only transformer architecture.\n",
    "##### For running 7b model change the model_id to \"mosaicml/mpt-7b-instruct\"\n",
    "##### https://huggingface.co/mosaicml/mpt-30b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a5a1d2-0915-41e0-91f7-be9c899be404",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/envs/genai23.08/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating an MPTForCausalLM model from /home/dshah47/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-30b-instruct/2abf1163dd8c9b11f07d805c06e6ec90a1f2037e/modeling_mpt.py\n",
      "You are using config.init_device='cuda:0', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████| 7/7 [00:37<00:00,  5.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = 'mosaicml/mpt-30b-instruct'\n",
    "\n",
    "## for 8bit use load in 8bit\n",
    "config = AutoConfig.from_pretrained(model_id,\n",
    "                                    trust_remote_code=True)\n",
    "# config.attn_config['attn_impl'] = 'triton'  # change this to use triton-based FlashAttention\n",
    "config.init_device = 'cuda:0' # For fast initialization directly on GPU!\n",
    "config.max_seq_len = 163844\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             trust_remote_code=True,\n",
    "                                             config=config,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             # load_in_8bit=True,\n",
    "                                             device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a50f18f6-c347-4ee6-bc67-7c0b6d4930f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Both `max_new_tokens` (=100) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Here is a recipe for vegan banana bread:\\n\\n1 cup mashed ripe banana (about 2 bananas)\\n\\n2 tablespoons canola oil, plus extra for greasing\\n\\n2 tablespoons plain soy yogurt\\n\\n3⁄4 cup sugar\\n\\n11⁄4 cups whole wheat flour\\n\\n1 cup all-purpose flour\\n\\n1 teaspoon baking powder\\n\\n1 teaspoon baking soda\\n\\n1⁄2 teaspoon salt\\n\\n1⁄2 cup walnuts, chopped\\n\\nPreheat the oven to 350°F. Grease a'}]\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_length=1024,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "    print(\n",
    "        pipe('Here is a recipe for vegan banana bread:\\n',\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            use_cache=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83dc09dd-43f3-4865-a83a-73c6b2eb0225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "def get_prompt(instruction):\n",
    "    prompt_template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n###Instruction\\n{instruction}\\n\\n### Response\\n\"\n",
    "    return prompt_template.format(instruction=instruction)\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "def generate(text):\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        response = pipe(prompt,\n",
    "                        max_new_tokens=256,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        top_p =0.95,\n",
    "                        top_k =  50,\n",
    "                        eos_token_id = 0,\n",
    "                        use_cache=True)[0]['generated_text']\n",
    "    return response\n",
    "\n",
    "def generate(text):\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=512,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.pad_token_id,\n",
    "                                 )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
    "        final_outputs = cut_off_text(final_outputs, '<|endoftext|>')\n",
    "        final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs#, outputs\n",
    "\n",
    "def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        # return assistant_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "744ca4e8-f3cb-423a-a031-7bba760fbcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpacas, vicunas and llamas are all types of South American camelids. They are all similar in\n",
      "appearance, with long necks, long eyelashes, and soft fur.  Alpacas are smaller than vicunas and\n",
      "llamas, and have a softer, denser fur. Alpacas are typically used for their fur, as it is very soft\n",
      "and warm.  Vicunas are larger than alpacas, and have a coarser, longer fur. Vicunas are typically\n",
      "used for their meat and fur.  Llamas are the largest of the three, and have a shaggier fur. Llamas\n",
      "are typically used for transportation, as they can carry heavy loads for long distances.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'What are the differences between alpacas, vicunas and llamas?'\n",
    "generated_text = generate(prompt)\n",
    "parse_text(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e6a7683-053e-4539-a651-f4190c7a4aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haiku is a form of Japanese poetry consisting of three lines with 17 syllables. The first line has 5\n",
      "syllables, the second line has 7 syllables, and the third line has 5 syllables.   A tweet is a form\n",
      "of social media post consisting of a maximum of 280 characters.   Therefore, it is not possible to\n",
      "write a full haiku in a single tweet. This is because, when formatted in the standard haiku style,\n",
      "the first line would take up 5 characters, the second line would take up 7 characters, and the third\n",
      "line would take up 5 characters. This is already 17 characters, and we haven't even included the end\n",
      "line break, which would take up 1 character.   So, in conclusion, it is not possible to write a full\n",
      "haiku in a single tweet.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Answer the following yes\\/no question by reasoning step-by-step. \\n Can you write a whole Haiku in a single tweet?'\n",
    "generated_text = generate(prompt)\n",
    "parse_text(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62283248-165c-4c46-8b15-c96049675240",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CodeLLaMa 34b instruct\n",
    "#### License A custom commercial license is available at: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf/blob/main/LICENSE.txt\n",
    "##### For running 7b / 13b models change the model_id to \"codellama/CodeLlama-7b-instruct-hf\" / \"codellama/CodeLlama-13b-instruct-hf\"\n",
    "##### https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d728ce9-7e20-439c-91fd-4d41b7b73a8e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "model_id = \"codellama/CodeLlama-34b-instruct-hf\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             trust_remote_code=True,\n",
    "                                             # torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\",\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e448b1f6-6787-4c13-a407-fe5a6504b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1,\n",
    "    device_map=\"auto\",\n",
    "    max_length=200,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a73c50-43c0-4b45-a50c-c6bf8cb414e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: import socket\n",
      "\n",
      "def ping_exponential_backoff(host: str):\n",
      "    \"\"\"\n",
      "    Ping a host with exponential backoff.\n",
      "    :param host: The host to ping.\n",
      "    :return: True if the host is reachable, False otherwise.\n",
      "    :raise socket.gaierror: If the hostname could not be resolved.\n",
      "    :raise socket.error: If a socket error occurred.\n",
      "    :raise OSError: If an OS error occurred.\n",
      "    :raise TypeError: If the host parameter is not a string.\n",
      "    :raise ValueError: If the host parameter is an empty string.\n",
      "    :raise Exception: If an unknown error occurred.\n",
      "    :raise BaseException: If an unknown error occurred.\n",
      "    :raise SystemExit: If an unknown error occurred.\n",
      "    :raise MemoryError: If an unknown error occurred.\n",
      "    :raise Warning: If an unknown error occurred.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequences = pipe(\n",
    "    'import socket\\n\\ndef ping_exponential_backoff(host: str):',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbcc4f4-4310-4ef8-9223-5aa724f32cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai23.08",
   "language": "python",
   "name": "genai23.08"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
